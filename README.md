# Sign Language Recognition System ğŸ¤Ÿ

A real-time sign language recognition system using deep learning and computer vision to detect and interpret sign language gestures.

## ğŸ“‹ Table of Contents
- Overview
- Features
- Project Structure
- Installation
- Usage
- Dataset
- Model Architecture
- Requirements
- Contributing
- License

## ğŸ¯ Overview

This project implements a machine learning-based sign language recognition system that can detect and interpret sign language gestures in real-time using a webcam. The system uses computer vision techniques and deep learning models to recognize hand gestures and translate them into text.

## âœ¨ Features

- Real-time sign language gesture recognition
- Custom dataset collection and preprocessing
- Deep learning model training pipeline
- Webcam integration for live detection
- Support for multiple sign language gestures
- Model performance visualization and metrics

## ğŸ“ Project Structure

```
Sign-Language-recognition-system/
â”‚
â”œâ”€â”€ __pycache__/          # Python cache files
â”œâ”€â”€ dataset/              # Training and testing datasets
â”œâ”€â”€ models/               # Saved trained models
â”œâ”€â”€ venv/                 # Virtual environment
â”‚
â”œâ”€â”€ data_collector.py     # Script to collect training data
â”œâ”€â”€ main.py               # Main application entry point
â”œâ”€â”€ model_trainer.py      # Model training script
â”œâ”€â”€ sign_detector.py      # Real-time sign detection
â”œâ”€â”€ signs_mapping.json    # Mapping of signs to labels
â”œâ”€â”€ requirements.txt      # Project dependencies
â”œâ”€â”€ .gitignore           # Git ignore file
â””â”€â”€ README.md            # Project documentation
```

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- Webcam (for real-time detection)
- pip package manager

### Steps

1. **Clone the repository**
```bash
git clone https://github.com/chand192004/Sign-Language-recognition-system.git
cd Sign-Language-recognition-system
```

2. **Create a virtual environment**
```bash
python -m venv venv

# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

## ğŸ’» Usage

### 1. Collect Training Data

First, collect gesture data for training:

```bash
python data_collector.py
```

Follow the on-screen instructions to record different sign language gestures.

### 2. Train the Model

Train the recognition model using collected data:

```bash
python model_trainer.py
```

The trained model will be saved in the `models/` directory.

### 3. Run Real-time Detection

Start the sign language detection system:

```bash
python main.py
```

Or use the dedicated sign detector:

```bash
python sign_detector.py
```

Press 'q' to quit the application.

## ğŸ“Š Dataset

The dataset consists of:
- Hand gesture images/videos
- Multiple sign language gestures
- Training and validation splits
- Augmented data for better model generalization

The `signs_mapping.json` file contains the mapping between gesture classes and their labels.

## ğŸ§  Model Architecture

The system uses a deep learning model (likely CNN-based) for gesture recognition:
- Input: Video frames or image sequences
- Feature extraction layers
- Classification layers
- Output: Predicted sign language gesture

Model files are saved in `.h5` format in the `models/` directory.

## ğŸ“¦ Requirements

Key dependencies include:
- TensorFlow/Keras - Deep learning framework
- OpenCV - Computer vision library
- NumPy - Numerical computing
- MediaPipe/CVZone - Hand tracking (if applicable)

See `requirements.txt` for complete list of dependencies.

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ‘¨â€ğŸ’» Author

**Chand** - [@chand192004](https://github.com/chand192004)


